{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2eef084-0e7d-47c1-b76a-fe952875a777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 19:21:12.665293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e1988-42ac-4367-b290-a51652306e16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61629c90-2b45-47d0-87f7-8918cdb379d8",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ddc97-ae40-410b-828c-8701ed386560",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625797c-7ec4-48c4-bc29-6f584538ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25660bec-dd12-45bc-b538-3452e29ca913",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_set = train_set.shuffle(len(X_train), seed=42)\n",
    "valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28a14d-1fef-4d14-9a28-c5ebe06f5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2422f4c6-8c8c-4baa-9b1b-4003bacd3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e4f7a-62f0-498e-981b-bcf74e02e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    return Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "                \"label\": Feature(int64_list=Int64List(value=[label])),\n",
    "            }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3417bb64-f941-41f6-817b-e3b39b6e07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in valid_set.take(1):\n",
    "    ex1 = create_example(image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7856039b-2c9c-48d2-a24c-5fb1b0cc991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bdc09a-af02-4935-90c8-5e02f47fb1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "def write_tfrecords(name, dataset, n_shards=10):\n",
    "    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n",
    "             for index in range(n_shards)]\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n",
    "                   for path in paths]\n",
    "        for index, (image, label) in dataset.enumerate():\n",
    "            shard = index % n_shards\n",
    "            example = create_example(image, label)\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda318ff-0278-48fd-b8cf-ab8353aefb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = write_tfrecords(\"./datasets/my_fashion_mnist/my_fashion_mnist.train\", train_set)\n",
    "valid_filepaths = write_tfrecords(\"./datasets/my_fashion_mnist/my_fashion_mnist.valid\", valid_set)\n",
    "test_filepaths = write_tfrecords(\"./datasets/my_fashion_mnist/my_fashion_mnist.test\", test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870f8ac-beba-40a9-9b40-040b75121d51",
   "metadata": {},
   "source": [
    "### b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e26268-e371-4b6b-bdec-ad83ac3594d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "    #image = tf.io.decode_jpeg(example[\"image\"])\n",
    "    image = tf.reshape(image, shape=[28, 28])\n",
    "    return image, example[\"label\"]\n",
    "\n",
    "def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n",
    "                  n_parse_threads=5, batch_size=32, cache=True):\n",
    "    dataset = tf.data.TFRecordDataset(filepaths,\n",
    "                                      num_parallel_reads=n_read_threads)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b6f5c-9e22-45a6-a4ac-d85753262e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
    "valid_set = mnist_dataset(valid_filepaths)\n",
    "test_set = mnist_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0106fd6-59ac-4d87-abe4-0b869fcb97a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in train_set.take(1):\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(X[i].numpy(), cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(str(y[i].numpy()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8214dc-169b-48bd-821c-be58dd179413",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "standardization = tf.keras.layers.Normalization(input_shape=[28, 28])\n",
    "\n",
    "sample_image_batches = train_set.take(100).map(lambda image, label: image)\n",
    "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n",
    "                               axis=0).astype(np.float32)\n",
    "standardization.adapt(sample_images)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    standardization,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df794f-e363-494a-a8e7-86d2f0d28b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b10959-3045-499d-8d17-c92576aafba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "logs = Path() / \"my_logs\" / \"run_\" / datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=logs, histogram_freq=1, profile_batch=10)\n",
    "\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set,\n",
    "          callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883064b6-69b5-44e2-a135-e63c3344c7c7",
   "metadata": {},
   "source": [
    "## Exercise 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79dc0f05-47bd-45bb-9fc3-6980f20d34ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('datasets/aclImdb')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "root = \"https://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "filename = \"aclImdb_v1.tar.gz\"\n",
    "filepath = tf.keras.utils.get_file(filename, root + filename, extract=True,\n",
    "                                   cache_dir=\".\")\n",
    "path = Path(filepath).with_name(\"aclImdb\")\n",
    "'''\n",
    "path = Path('datasets/aclImdb')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc4a7f8-e193-445f-8df8-8ce964406410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(path, level=0, indent=4, max_files=3):\n",
    "    if level == 0:\n",
    "        print(f\"{path}/\")\n",
    "        level += 1\n",
    "    sub_paths = sorted(path.iterdir())\n",
    "    sub_dirs = [sub_path for sub_path in sub_paths if sub_path.is_dir()]\n",
    "    filepaths = [sub_path for sub_path in sub_paths if not sub_path in sub_dirs]\n",
    "    indent_str = \" \" * indent * level\n",
    "    for sub_dir in sub_dirs:\n",
    "        print(f\"{indent_str}{sub_dir.name}/\")\n",
    "        tree(sub_dir,  level + 1, indent)\n",
    "    for filepath in filepaths[:max_files]:\n",
    "        print(f\"{indent_str}{filepath.name}\")\n",
    "    if len(filepaths) > max_files:\n",
    "        print(f\"{indent_str}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf170097-050d-4bc5-8fe4-dc94f8489d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/aclImdb/\n",
      "    test/\n",
      "        neg/\n",
      "            0_2.txt\n",
      "            10000_4.txt\n",
      "            10001_1.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_10.txt\n",
      "            10000_7.txt\n",
      "            10001_9.txt\n",
      "            ...\n",
      "        labeledBow.feat\n",
      "        urls_neg.txt\n",
      "        urls_pos.txt\n",
      "    train/\n",
      "        neg/\n",
      "            0_3.txt\n",
      "            10000_4.txt\n",
      "            10001_4.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_9.txt\n",
      "            10000_8.txt\n",
      "            10001_10.txt\n",
      "            ...\n",
      "        unsup/\n",
      "            0_0.txt\n",
      "            10000_0.txt\n",
      "            10001_0.txt\n",
      "            ...\n",
      "        labeledBow.feat\n",
      "        unsupBow.feat\n",
      "        urls_neg.txt\n",
      "        ...\n",
      "    README\n",
      "    imdb.vocab\n",
      "    imdbEr.txt\n"
     ]
    }
   ],
   "source": [
    "tree(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474aa949-3cdf-4982-b53a-eb3c52636f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def review_paths(dirpath):\n",
    "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
    "\n",
    "train_pos = review_paths(path / \"train\" / \"pos\")\n",
    "train_neg = review_paths(path / \"train\" / \"neg\")\n",
    "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
    "test_valid_neg = review_paths(path / \"test\" / \"neg\")\n",
    "\n",
    "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e450d-d2fa-470b-953d-d55bc0bb3388",
   "metadata": {},
   "source": [
    "### b. Split the test set into a validation set (15,000) and a test set (10,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30f10f4-5281-4b7f-bedd-5d281894314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_valid_pos)\n",
    "\n",
    "test_pos = test_valid_pos[:5000]\n",
    "test_neg = test_valid_neg[:5000]\n",
    "valid_pos = test_valid_pos[5000:]\n",
    "valid_neg = test_valid_neg[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b93c3-f1ad-4a03-bb8a-8deb73e6bdf2",
   "metadata": {},
   "source": [
    "### c. Use tf.data to create an efficient dataset for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1b5d4b-53a1-4881-915a-d4dd8103fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath) as review_file:\n",
    "                reviews.append(review_file.read())\n",
    "            labels.append(label)\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.constant(reviews), tf.constant(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a269636e-65c6-49c7-8add-f51474e15c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 19:22:11.757201: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "for X, y in imdb_dataset(train_pos, train_neg).take(1):\n",
    "    print(X)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d02b1a1-5aea-4781-b6bf-043cc3c548bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.37 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(1): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f2b7137-9ac5-4230-9f28-f55ed9a755cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
    "    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n",
    "                                          num_parallel_reads=n_read_threads)\n",
    "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
    "    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n",
    "                                          num_parallel_reads=n_read_threads)\n",
    "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
    "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b84de04-41dd-4b0e-befb-68bddea766c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/chenxu/opt/miniconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "tf.Tensor(b'For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.', shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b'Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV\\'s \"Flamingo Road\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the Gateway to Hell! The scenes with Raines modeling are very well captured, the mood music is perfect, Deborah Raffin is charming as Cristina\\'s pal, but when Raines moves into a creepy Brooklyn Heights brownstone (inhabited by a blind priest on the top floor), things really start cooking. The neighbors, including a fantastically wicked Burgess Meredith and kinky couple Sylvia Miles & Beverly D\\'Angelo, are a diabolical lot, and Eli Wallach is great fun as a wily police detective. The movie is nearly a cross-pollination of \"Rosemary\\'s Baby\" and \"The Exorcist\"--but what a combination! Based on the best-seller by Jeffrey Konvitz, \"The Sentinel\" is entertainingly spooky, full of shocks brought off well by director Michael Winner, who mounts a thoughtfully downbeat ending with skill. ***1/2 from ****', shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b'A solid, if unremarkable film. Matthau, as Einstein, was wonderful. My favorite part, and the only thing that would make me go out of my way to see this again, was the wonderful scene with the physicists playing badmitton, I loved the sweaters and the conversation while they waited for Robbins to retrieve the birdie.', shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X, y in imdb_dataset(train_pos, train_neg).take(3):\n",
    "    print(X)\n",
    "    print(y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae3b9f0b-d5f5-480b-86f2-4f0811b2daef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.91 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(1): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63a7b92a-1898-4fec-967b-59af59709fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000, seed=42)\n",
    "train_set = train_set.batch(batch_size).prefetch(1)\n",
    "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
    "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48319b5d-94aa-4064-9f0e-3ef1be80d343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'A film starring Salma Hayek and Colin Farrell, two respected and talented actors, sounds like a great idea. An independent film sounds even better. The studios will control less of the content allowing the actors and writers and director more creativity.<br /><br />But then why is this movie so bland? Ask the dust.<br /><br />This film assumes right off the bat that we are deeply invested in the characters. No one is given a proper back story, so we don\\'t ever know why the characters act the way they do.<br /><br />Explanations for physical and emotional scars are left to our imagination, if you still have one left at the end of this movie.<br /><br />I told a friend that I went to see this film, and that I thought it was awful.<br /><br />Her question: \"Not even Colin Farrell could save it?\" My response: \"Not even Colin Farrell\\'s ass could save it.\"'\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_set.take(1):\n",
    "    print(X.numpy()[0])\n",
    "    print(y.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de95b08-ba05-41c5-986d-d9f6ad7c049e",
   "metadata": {},
   "source": [
    "### d. Create a binary classification model, using a TextVectorization layer to preprocess each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d42b1579-1779-46e7-a31f-ed5ba7d129c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 1000\n",
    "sample_reviews = train_set.map(lambda review, label: review)\n",
    "text_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens, output_mode=\"tf_idf\")\n",
    "text_vectorization.adapt(sample_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e40c7052-8736-4075-9db4-6c4ef4cf71c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cd77dd8-1b87-4cc6-b8de-71bc4c4aaf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 5s 4ms/step - loss: 0.4151 - accuracy: 0.8249 - val_loss: 0.4142 - val_accuracy: 0.8342\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.3549 - accuracy: 0.8548 - val_loss: 0.4568 - val_accuracy: 0.8192\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3120 - accuracy: 0.8726 - val_loss: 0.3660 - val_accuracy: 0.8479\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2609 - accuracy: 0.8943 - val_loss: 0.3525 - val_accuracy: 0.8490\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1969 - accuracy: 0.9235 - val_loss: 0.3978 - val_accuracy: 0.8433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f882de687d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorization,\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344eb1ac-ec21-4587-8e3f-81a51a50cb44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
